"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[7473],{741:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Vision-Language-Action/Multi-Modal-Interaction/multi-modal-interaction","title":"Multi-Modal Interaction","description":"Introduction","source":"@site/docs/05-Vision-Language-Action/04-Multi-Modal-Interaction/index.md","sourceDirName":"05-Vision-Language-Action/04-Multi-Modal-Interaction","slug":"/module-05/multi-modal-interaction","permalink":"/physical-ai-hackathon/docs/module-05/multi-modal-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/05-Vision-Language-Action/04-Multi-Modal-Interaction/index.md","tags":[],"version":"current","frontMatter":{"id":"multi-modal-interaction","title":"Multi-Modal Interaction","slug":"/module-05/multi-modal-interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-hackathon/docs/module-05/cognitive-planning-llms"},"next":{"title":"End-to-End VLA Implementation","permalink":"/physical-ai-hackathon/docs/module-05/end-to-end-vla"}}');var i=r(4848),o=r(8453);const s={id:"multi-modal-interaction",title:"Multi-Modal Interaction",slug:"/module-05/multi-modal-interaction"},a="Multi-Modal Interaction",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Multi-Modal Architecture",id:"multi-modal-architecture",level:2},{value:"Complete Multi-Modal System",id:"complete-multi-modal-system",level:2},{value:"Gesture Recognition",id:"gesture-recognition",level:2},{value:"Visual Grounding",id:"visual-grounding",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"multi-modal-interaction",children:"Multi-Modal Interaction"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Multi-modal interaction"})," combines vision, language, and action to enable natural human-robot communication. This chapter covers integrating multiple sensory modalities for richer robot understanding and interaction."]}),"\n",(0,i.jsx)(e.h2,{id:"multi-modal-architecture",children:"Multi-Modal Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Input Modalities"\r\n        VOICE[Voice Input]\r\n        GESTURE[Gestures]\r\n        CAMERA[Visual Scene]\r\n        TOUCH[Touch/Force]\r\n    end\r\n    \r\n    subgraph "Processing"\r\n        ASR[Speech Recognition]\r\n        VISION[Computer Vision]\r\n        POSE[Pose Estimation]\r\n        FORCE_PROC[Force Processing]\r\n    end\r\n    \r\n    subgraph "Fusion"\r\n        FUSION[Multi-Modal Fusion]\r\n        CONTEXT[Context Manager]\r\n    end\r\n    \r\n    subgraph "Understanding"\r\n        INTENT[Intent Recognition]\r\n        GROUNDING[Language Grounding]\r\n    end\r\n    \r\n    subgraph "Output"\r\n        ACTIONS[Robot Actions]\r\n        SPEECH[Speech Output]\r\n        DISPLAY[Visual Feedback]\r\n    end\r\n    \r\n    VOICE --\x3e ASR\r\n    GESTURE --\x3e POSE\r\n    CAMERA --\x3e VISION\r\n    TOUCH --\x3e FORCE_PROC\r\n    \r\n    ASR --\x3e FUSION\r\n    POSE --\x3e FUSION\r\n    VISION --\x3e FUSION\r\n    FORCE_PROC --\x3e FUSION\r\n    \r\n    FUSION --\x3e CONTEXT\r\n    CONTEXT --\x3e INTENT\r\n    INTENT --\x3e GROUNDING\r\n    GROUNDING --\x3e ACTIONS\r\n    GROUNDING --\x3e SPEECH\r\n    GROUNDING --\x3e DISPLAY\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"complete-multi-modal-system",children:"Complete Multi-Modal System"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nMulti-modal interaction system for humanoid robots.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, JointState\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport openai\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport whisper\r\nfrom transformers import pipeline\r\n\r\nclass MultiModalInteraction(Node):\r\n    """\r\n    Integrates vision, language, and action for natural interaction.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'multi_modal_interaction\')\r\n        \r\n        # Initialize AI models\r\n        self.setup_ai_models()\r\n        \r\n        # State\r\n        self.current_scene = None\r\n        self.detected_objects = []\r\n        self.user_pose = None\r\n        self.conversation_history = []\r\n        \r\n        # ROS interfaces\r\n        self.setup_ros_interfaces()\r\n        \r\n        self.get_logger().info(\'Multi-Modal Interaction initialized\')\r\n    \r\n    def setup_ai_models(self):\r\n        """Initialize AI models"""\r\n        # Speech recognition\r\n        self.whisper_model = whisper.load_model("base")\r\n        \r\n        # Vision model (CLIP for image understanding)\r\n        self.vision_model = pipeline(\r\n            "zero-shot-image-classification",\r\n            model="openai/clip-vit-base-patch32"\r\n        )\r\n        \r\n        # LLM for reasoning\r\n        self.llm_client = openai.Client(api_key=os.getenv(\'OPENAI_API_KEY\'))\r\n        \r\n        # Pose estimation\r\n        self.pose_detector = cv2.dnn.readNetFromTensorflow(\'pose_model.pb\')\r\n    \r\n    def setup_ros_interfaces(self):\r\n        """Setup ROS publishers and subscribers"""\r\n        # Subscribe to camera\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n        \r\n        # Subscribe to audio\r\n        self.audio_sub = self.create_subscription(\r\n            String, \'/audio/transcription\', self.audio_callback, 10\r\n        )\r\n        \r\n        # Publish robot actions\r\n        self.action_pub = self.create_publisher(\r\n            String, \'/robot/action_command\', 10\r\n        )\r\n        \r\n        # Publish speech\r\n        self.speech_pub = self.create_publisher(\r\n            String, \'/robot/speech\', 10\r\n        )\r\n        \r\n        # CV Bridge\r\n        self.bridge = CvBridge()\r\n    \r\n    def image_callback(self, msg):\r\n        """Process visual input"""\r\n        # Convert to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n        \r\n        # Detect objects\r\n        self.detected_objects = self.detect_objects(cv_image)\r\n        \r\n        # Detect human pose\r\n        self.user_pose = self.detect_human_pose(cv_image)\r\n        \r\n        # Store current scene\r\n        self.current_scene = cv_image\r\n    \r\n    def audio_callback(self, msg):\r\n        """Process voice input"""\r\n        user_speech = msg.data\r\n        \r\n        # Process multi-modal command\r\n        self.process_multi_modal_command(user_speech)\r\n    \r\n    def detect_objects(self, image):\r\n        """\r\n        Detect objects in image using CLIP.\r\n        \r\n        Args:\r\n            image: OpenCV image\r\n            \r\n        Returns:\r\n            list: Detected objects with labels and confidence\r\n        """\r\n        # Convert to PIL\r\n        from PIL import Image as PILImage\r\n        pil_image = PILImage.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n        \r\n        # Candidate labels\r\n        candidate_labels = [\r\n            "cup", "bottle", "book", "phone", "laptop",\r\n            "chair", "table", "person", "door", "window"\r\n        ]\r\n        \r\n        # Classify\r\n        results = self.vision_model(pil_image, candidate_labels=candidate_labels)\r\n        \r\n        # Filter by confidence\r\n        objects = [r for r in results if r[\'score\'] > 0.3]\r\n        \r\n        return objects\r\n    \r\n    def detect_human_pose(self, image):\r\n        """\r\n        Detect human pose for gesture recognition.\r\n        \r\n        Args:\r\n            image: OpenCV image\r\n            \r\n        Returns:\r\n            dict: Pose keypoints\r\n        """\r\n        # Preprocess\r\n        blob = cv2.dnn.blobFromImage(\r\n            image, 1.0, (368, 368),\r\n            (127.5, 127.5, 127.5), swapRB=True, crop=False\r\n        )\r\n        \r\n        # Inference\r\n        self.pose_detector.setInput(blob)\r\n        output = self.pose_detector.forward()\r\n        \r\n        # Extract keypoints\r\n        keypoints = self.extract_keypoints(output)\r\n        \r\n        # Recognize gesture\r\n        gesture = self.recognize_gesture(keypoints)\r\n        \r\n        return {\'keypoints\': keypoints, \'gesture\': gesture}\r\n    \r\n    def recognize_gesture(self, keypoints):\r\n        """\r\n        Recognize gesture from pose keypoints.\r\n        \r\n        Args:\r\n            keypoints: Pose keypoints\r\n            \r\n        Returns:\r\n            str: Gesture name\r\n        """\r\n        # Simple gesture recognition\r\n        if keypoints is None:\r\n            return None\r\n        \r\n        # Check for pointing gesture\r\n        if self.is_pointing(keypoints):\r\n            return \'pointing\'\r\n        \r\n        # Check for waving\r\n        if self.is_waving(keypoints):\r\n            return \'waving\'\r\n        \r\n        # Check for beckoning\r\n        if self.is_beckoning(keypoints):\r\n            return \'beckoning\'\r\n        \r\n        return None\r\n    \r\n    def is_pointing(self, keypoints):\r\n        """Check if user is pointing"""\r\n        # Simplified: check if arm is extended\r\n        # Real implementation would be more sophisticated\r\n        return False  # Placeholder\r\n    \r\n    def process_multi_modal_command(self, speech):\r\n        """\r\n        Process command using speech, vision, and gestures.\r\n        \r\n        Args:\r\n            speech: User\'s spoken command\r\n        """\r\n        # Build multi-modal context\r\n        context = self.build_context(speech)\r\n        \r\n        # Get LLM interpretation\r\n        interpretation = self.interpret_with_llm(context)\r\n        \r\n        # Execute action\r\n        self.execute_interpreted_action(interpretation)\r\n    \r\n    def build_context(self, speech):\r\n        """\r\n        Build multi-modal context.\r\n        \r\n        Args:\r\n            speech: User speech\r\n            \r\n        Returns:\r\n            dict: Multi-modal context\r\n        """\r\n        context = {\r\n            \'speech\': speech,\r\n            \'visible_objects\': [obj[\'label\'] for obj in self.detected_objects],\r\n            \'user_gesture\': self.user_pose[\'gesture\'] if self.user_pose else None,\r\n            \'conversation_history\': self.conversation_history[-5:]  # Last 5 turns\r\n        }\r\n        \r\n        return context\r\n    \r\n    def interpret_with_llm(self, context):\r\n        """\r\n        Use LLM to interpret multi-modal input.\r\n        \r\n        Args:\r\n            context: Multi-modal context\r\n            \r\n        Returns:\r\n            dict: Interpreted action\r\n        """\r\n        prompt = f"""\r\nYou are a humanoid robot assistant. Interpret the user\'s multi-modal command.\r\n\r\nUser said: "{context[\'speech\']}"\r\nVisible objects: {context[\'visible_objects\']}\r\nUser gesture: {context[\'user_gesture\']}\r\n\r\nPrevious conversation:\r\n{self.format_conversation_history(context[\'conversation_history\'])}\r\n\r\nDetermine:\r\n1. What object is the user referring to?\r\n2. What action should the robot take?\r\n3. Any clarification needed?\r\n\r\nRespond in JSON format:\r\n{{\r\n  "target_object": "object name or null",\r\n  "action": "action to take",\r\n  "clarification_needed": "question to ask or null",\r\n  "reasoning": "explanation"\r\n}}\r\n"""\r\n        \r\n        response = self.llm_client.chat.completions.create(\r\n            model=\'gpt-4\',\r\n            messages=[\r\n                {\'role\': \'system\', \'content\': \'You are a helpful robot assistant.\'},\r\n                {\'role\': \'user\', \'content\': prompt}\r\n            ],\r\n            response_format={\'type\': \'json_object\'}\r\n        )\r\n        \r\n        import json\r\n        interpretation = json.loads(response.choices[0].message.content)\r\n        \r\n        return interpretation\r\n    \r\n    def execute_interpreted_action(self, interpretation):\r\n        """\r\n        Execute action based on interpretation.\r\n        \r\n        Args:\r\n            interpretation: LLM interpretation\r\n        """\r\n        # Check if clarification needed\r\n        if interpretation[\'clarification_needed\']:\r\n            self.speak(interpretation[\'clarification_needed\'])\r\n            return\r\n        \r\n        # Execute action\r\n        action = interpretation[\'action\']\r\n        target = interpretation[\'target_object\']\r\n        \r\n        self.get_logger().info(f\'Executing: {action} on {target}\')\r\n        \r\n        # Publish action command\r\n        cmd_msg = String()\r\n        cmd_msg.data = f"{action}:{target}"\r\n        self.action_pub.publish(cmd_msg)\r\n        \r\n        # Provide feedback\r\n        self.speak(f"I will {action} the {target}.")\r\n        \r\n        # Update conversation history\r\n        self.conversation_history.append({\r\n            \'user\': interpretation[\'speech\'],\r\n            \'robot\': f"I will {action} the {target}."\r\n        })\r\n    \r\n    def speak(self, text):\r\n        """\r\n        Make robot speak.\r\n        \r\n        Args:\r\n            text: Text to speak\r\n        """\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_pub.publish(msg)\r\n        \r\n        self.get_logger().info(f\'Robot says: {text}\')\r\n    \r\n    def format_conversation_history(self, history):\r\n        """Format conversation history for LLM"""\r\n        formatted = []\r\n        for turn in history:\r\n            formatted.append(f"User: {turn[\'user\']}")\r\n            formatted.append(f"Robot: {turn[\'robot\']}")\r\n        return \'\\n\'.join(formatted)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    interaction = MultiModalInteraction()\r\n    \r\n    try:\r\n        rclpy.spin(interaction)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        interaction.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"gesture-recognition",children:"Gesture Recognition"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class GestureRecognizer:\r\n    \"\"\"\r\n    Recognizes common gestures from pose keypoints.\r\n    \"\"\"\r\n    \r\n    def __init__(self):\r\n        self.gesture_templates = {\r\n            'pointing': self.check_pointing,\r\n            'waving': self.check_waving,\r\n            'thumbs_up': self.check_thumbs_up,\r\n            'stop': self.check_stop_gesture\r\n        }\r\n    \r\n    def recognize(self, keypoints):\r\n        \"\"\"\r\n        Recognize gesture from keypoints.\r\n        \r\n        Args:\r\n            keypoints: Pose keypoints\r\n            \r\n        Returns:\r\n            str: Gesture name or None\r\n        \"\"\"\r\n        for gesture_name, check_func in self.gesture_templates.items():\r\n            if check_func(keypoints):\r\n                return gesture_name\r\n        \r\n        return None\r\n    \r\n    def check_pointing(self, keypoints):\r\n        \"\"\"Check for pointing gesture\"\"\"\r\n        # Get relevant keypoints\r\n        shoulder = keypoints['right_shoulder']\r\n        elbow = keypoints['right_elbow']\r\n        wrist = keypoints['right_wrist']\r\n        \r\n        # Check if arm is extended\r\n        arm_angle = self.calculate_angle(shoulder, elbow, wrist)\r\n        \r\n        # Pointing: arm mostly straight (angle > 150\xb0)\r\n        return arm_angle > 150\r\n    \r\n    def check_waving(self, keypoints):\r\n        \"\"\"Check for waving gesture\"\"\"\r\n        # Detect periodic hand motion\r\n        # (requires temporal information)\r\n        pass\r\n    \r\n    def calculate_angle(self, p1, p2, p3):\r\n        \"\"\"Calculate angle between three points\"\"\"\r\n        import math\r\n        \r\n        v1 = np.array([p1['x'] - p2['x'], p1['y'] - p2['y']])\r\n        v2 = np.array([p3['x'] - p2['x'], p3['y'] - p2['y']])\r\n        \r\n        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\r\n        angle = math.acos(np.clip(cos_angle, -1.0, 1.0))\r\n        \r\n        return math.degrees(angle)\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"visual-grounding",children:"Visual Grounding"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class VisualGrounding:\r\n    \"\"\"\r\n    Grounds language references to visual objects.\r\n    \"\"\"\r\n    \r\n    def ground_reference(self, text, detected_objects, user_pose):\r\n        \"\"\"\r\n        Ground linguistic reference to object.\r\n        \r\n        Args:\r\n            text: User's reference (e.g., \"that cup\", \"the red one\")\r\n            detected_objects: List of detected objects\r\n            user_pose: User's pose (for pointing direction)\r\n            \r\n        Returns:\r\n            Object that user is referring to\r\n        \"\"\"\r\n        # Extract referring expression\r\n        if 'that' in text or 'this' in text:\r\n            # Use pointing gesture\r\n            if user_pose and user_pose['gesture'] == 'pointing':\r\n                return self.get_pointed_object(user_pose, detected_objects)\r\n        \r\n        # Use color/attribute\r\n        if 'red' in text:\r\n            return self.find_by_color(detected_objects, 'red')\r\n        \r\n        # Use spatial relation\r\n        if 'left' in text:\r\n            return self.find_leftmost(detected_objects)\r\n        \r\n        # Default: closest object\r\n        return self.find_closest(detected_objects)\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-modal interaction"})," combines vision, language, and gestures"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context fusion"})," integrates multiple input modalities"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Visual grounding"})," connects language to perceived objects"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Gesture recognition"})," enables non-verbal communication"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"LLMs"})," interpret multi-modal context"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Multi-modal interaction enables more natural and intuitive human-robot communication."}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://openai.com/research/clip",children:"CLIP: Connecting Text and Images"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2206.06488",children:"Multi-Modal Learning"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1808.08089",children:"Visual Grounding"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2004.00433",children:"Gesture Recognition"})}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);