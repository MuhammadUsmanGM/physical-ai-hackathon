"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[9122],{4295:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Introduction-to-Physical-AI/Sensor-Systems/sensor-systems","title":"Sensor Systems Deep Dive","description":"Introduction","source":"@site/docs/01-Introduction-to-Physical-AI/03-Sensor-Systems/index.md","sourceDirName":"01-Introduction-to-Physical-AI/03-Sensor-Systems","slug":"/module-01/sensor-systems","permalink":"/physical-ai-hackathon/docs/module-01/sensor-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/01-Introduction-to-Physical-AI/03-Sensor-Systems/index.md","tags":[],"version":"current","frontMatter":{"id":"sensor-systems","title":"Sensor Systems Deep Dive","slug":"/module-01/sensor-systems"},"sidebar":"tutorialSidebar","previous":{"title":"Why Physical AI Matters","permalink":"/physical-ai-hackathon/docs/module-01/why-physical-ai-matters"},"next":{"title":"Embodiment Hypothesis & Physical Reasoning","permalink":"/physical-ai-hackathon/docs/module-01/embodiment-physical-reasoning"}}');var i=r(4848),t=r(8453);const a={id:"sensor-systems",title:"Sensor Systems Deep Dive",slug:"/module-01/sensor-systems"},l="Sensor Systems Deep Dive",o={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Sensor Overview",id:"sensor-overview",level:2},{value:"1. LiDAR (Light Detection and Ranging)",id:"1-lidar-light-detection-and-ranging",level:2},{value:"Principle of Operation",id:"principle-of-operation",level:3},{value:"Types of LiDAR",id:"types-of-lidar",level:3},{value:"Data Format",id:"data-format",level:3},{value:"Integration Example",id:"integration-example",level:3},{value:"Common LiDAR Models",id:"common-lidar-models",level:3},{value:"2. Cameras",id:"2-cameras",level:2},{value:"Types of Cameras",id:"types-of-cameras",level:3},{value:"RGB Cameras",id:"rgb-cameras",level:4},{value:"Depth Cameras",id:"depth-cameras",level:4},{value:"Thermal Cameras",id:"thermal-cameras",level:4},{value:"Camera Parameters",id:"camera-parameters",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Recommended Cameras for Humanoids",id:"recommended-cameras-for-humanoids",level:3},{value:"3. IMU (Inertial Measurement Unit)",id:"3-imu-inertial-measurement-unit",level:2},{value:"Components",id:"components",level:3},{value:"Data Format",id:"data-format-1",level:3},{value:"Orientation Estimation",id:"orientation-estimation",level:3},{value:"IMU Calibration",id:"imu-calibration",level:3},{value:"Recommended IMUs",id:"recommended-imus",level:3},{value:"4. Force/Torque Sensors",id:"4-forcetorque-sensors",level:2},{value:"Purpose",id:"purpose",level:3},{value:"Data Format",id:"data-format-2",level:3},{value:"Integration Example",id:"integration-example-1",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Example: IMU + Encoder Fusion",id:"example-imu--encoder-fusion",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sensor-systems-deep-dive",children:"Sensor Systems Deep Dive"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["Sensors are the ",(0,i.jsx)(n.strong,{children:"eyes, ears, and nervous system"})," of humanoid robots. They provide the data needed for perception, localization, and control. This chapter covers the four critical sensor types mentioned in the course: ",(0,i.jsx)(n.strong,{children:"LiDAR, Cameras, IMUs, and Force/Torque Sensors"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"sensor-overview",children:"Sensor Overview"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Exteroceptive Sensors (External Environment)"\r\n        LIDAR[LiDAR]\r\n        CAMERA[Cameras]\r\n        DEPTH[Depth Cameras]\r\n    end\r\n    \r\n    subgraph "Proprioceptive Sensors (Internal State)"\r\n        IMU[IMU]\r\n        ENCODER[Joint Encoders]\r\n        FORCE[Force/Torque Sensors]\r\n    end\r\n    \r\n    subgraph "Perception Pipeline"\r\n        LIDAR --\x3e MAPPING[3D Mapping]\r\n        CAMERA --\x3e VISION[Computer Vision]\r\n        DEPTH --\x3e POINTCLOUD[Point Clouds]\r\n        IMU --\x3e ORIENTATION[Orientation Estimation]\r\n        ENCODER --\x3e KINEMATICS[Forward Kinematics]\r\n        FORCE --\x3e CONTACT[Contact Detection]\r\n    end\r\n    \r\n    subgraph "Robot Control"\r\n        MAPPING --\x3e NAV[Navigation]\r\n        VISION --\x3e MANIP[Manipulation]\r\n        POINTCLOUD --\x3e OBSTACLE[Obstacle Avoidance]\r\n        ORIENTATION --\x3e BALANCE[Balance Control]\r\n        KINEMATICS --\x3e MOTION[Motion Planning]\r\n        CONTACT --\x3e COMPLIANCE[Compliant Control]\r\n    end\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"1-lidar-light-detection-and-ranging",children:"1. LiDAR (Light Detection and Ranging)"}),"\n",(0,i.jsx)(n.h3,{id:"principle-of-operation",children:"Principle of Operation"}),"\n",(0,i.jsx)(n.p,{children:"LiDAR measures distance by emitting laser pulses and measuring time-of-flight:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Distance = (Speed of Light \xd7 Time) / 2\n"})}),"\n",(0,i.jsx)(n.h3,{id:"types-of-lidar",children:"Types of LiDAR"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Scan Pattern"}),(0,i.jsx)(n.th,{children:"Range"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"2D LiDAR"})}),(0,i.jsx)(n.td,{children:"Single plane"}),(0,i.jsx)(n.td,{children:"0.1-30m"}),(0,i.jsx)(n.td,{children:"Floor-level obstacle detection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"3D LiDAR"})}),(0,i.jsx)(n.td,{children:"Multi-plane or rotating"}),(0,i.jsx)(n.td,{children:"0.1-100m"}),(0,i.jsx)(n.td,{children:"Full 3D mapping"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Solid-State"})}),(0,i.jsx)(n.td,{children:"Electronic scanning"}),(0,i.jsx)(n.td,{children:"0.1-200m"}),(0,i.jsx)(n.td,{children:"Automotive, no moving parts"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"data-format",children:"Data Format"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2D LiDAR (LaserScan):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# ROS 2 sensor_msgs/LaserScan\r\n{\r\n    'header': {\r\n        'stamp': timestamp,\r\n        'frame_id': 'laser_link'\r\n    },\r\n    'angle_min': -3.14,      # Start angle (radians)\r\n    'angle_max': 3.14,       # End angle\r\n    'angle_increment': 0.01, # Angular resolution\r\n    'range_min': 0.1,        # Minimum range (meters)\r\n    'range_max': 30.0,       # Maximum range\r\n    'ranges': [2.5, 2.6, ...],  # Distance measurements\r\n    'intensities': [100, 95, ...]  # Reflectivity (optional)\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3D LiDAR (PointCloud2):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# ROS 2 sensor_msgs/PointCloud2\r\n# Each point: (x, y, z, intensity)\r\npoints = [\r\n    [1.2, 0.5, 0.3, 100],\r\n    [1.3, 0.5, 0.3, 95],\r\n    ...\r\n]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"integration-example",children:"Integration Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nimport numpy as np\r\n\r\nclass LidarProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_processor')\r\n        \r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n    \r\n    def lidar_callback(self, msg):\r\n        \"\"\"Process LiDAR scan data\"\"\"\r\n        ranges = np.array(msg.ranges)\r\n        \r\n        # Filter invalid readings\r\n        valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\r\n        \r\n        # Detect obstacles\r\n        min_distance = np.min(valid_ranges)\r\n        \r\n        if min_distance < 0.5:  # 50cm threshold\r\n            self.get_logger().warn(f'Obstacle detected at {min_distance:.2f}m')\r\n        \r\n        # Convert to Cartesian coordinates\r\n        angles = np.arange(msg.angle_min, msg.angle_max, msg.angle_increment)\r\n        x = ranges * np.cos(angles)\r\n        y = ranges * np.sin(angles)\r\n        \r\n        # Further processing...\n"})}),"\n",(0,i.jsx)(n.h3,{id:"common-lidar-models",children:"Common LiDAR Models"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Range"}),(0,i.jsx)(n.th,{children:"Price Range"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Hokuyo UST-10LX"})}),(0,i.jsx)(n.td,{children:"2D"}),(0,i.jsx)(n.td,{children:"10m"}),(0,i.jsx)(n.td,{children:"$1,000"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"SICK TiM561"})}),(0,i.jsx)(n.td,{children:"2D"}),(0,i.jsx)(n.td,{children:"10m"}),(0,i.jsx)(n.td,{children:"$1,500"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Velodyne VLP-16"})}),(0,i.jsx)(n.td,{children:"3D (16 channels)"}),(0,i.jsx)(n.td,{children:"100m"}),(0,i.jsx)(n.td,{children:"$4,000"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Ouster OS1-64"})}),(0,i.jsx)(n.td,{children:"3D (64 channels)"}),(0,i.jsx)(n.td,{children:"120m"}),(0,i.jsx)(n.td,{children:"$12,000"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"2-cameras",children:"2. Cameras"}),"\n",(0,i.jsx)(n.h3,{id:"types-of-cameras",children:"Types of Cameras"}),"\n",(0,i.jsx)(n.h4,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose:"})," Color vision, object recognition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resolution:"})," 640x480 to 4K"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Frame Rate:"})," 30-120 FPS"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Technologies:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stereo:"})," Two cameras, triangulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Structured Light:"})," Project pattern, measure distortion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Time-of-Flight (ToF):"})," Measure light travel time"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"thermal-cameras",children:"Thermal Cameras"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose:"})," Heat detection, night vision"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Cases:"})," Human detection, equipment monitoring"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"camera-parameters",children:"Camera Parameters"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Camera intrinsic parameters\r\ncamera_matrix = np.array([\r\n    [fx, 0,  cx],  # fx: focal length in x\r\n    [0,  fy, cy],  # fy: focal length in y\r\n    [0,  0,  1 ]   # cx, cy: principal point\r\n])\r\n\r\n# Distortion coefficients\r\ndist_coeffs = np.array([k1, k2, p1, p2, k3])\r\n# k1, k2, k3: radial distortion\r\n# p1, p2: tangential distortion\n"})}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass CameraProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'camera_processor\')\r\n        \r\n        self.bridge = CvBridge()\r\n        \r\n        # Subscribe to camera topics\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/camera_info\',\r\n            self.info_callback,\r\n            10\r\n        )\r\n        \r\n        self.camera_matrix = None\r\n        self.dist_coeffs = None\r\n    \r\n    def info_callback(self, msg):\r\n        """Store camera calibration"""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.dist_coeffs = np.array(msg.d)\r\n    \r\n    def image_callback(self, msg):\r\n        """Process camera image"""\r\n        # Convert ROS Image to OpenCV format\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n        \r\n        # Undistort image\r\n        if self.camera_matrix is not None:\r\n            cv_image = cv2.undistort(\r\n                cv_image,\r\n                self.camera_matrix,\r\n                self.dist_coeffs\r\n            )\r\n        \r\n        # Object detection, etc.\r\n        self.detect_objects(cv_image)\r\n    \r\n    def detect_objects(self, image):\r\n        """Example: Simple color-based detection"""\r\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\r\n        \r\n        # Detect red objects\r\n        lower_red = np.array([0, 100, 100])\r\n        upper_red = np.array([10, 255, 255])\r\n        mask = cv2.inRange(hsv, lower_red, upper_red)\r\n        \r\n        # Find contours\r\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n        \r\n        for contour in contours:\r\n            if cv2.contourArea(contour) > 500:\r\n                x, y, w, h = cv2.boundingRect(contour)\r\n                self.get_logger().info(f\'Red object detected at ({x}, {y})\')\n'})}),"\n",(0,i.jsx)(n.h3,{id:"recommended-cameras-for-humanoids",children:"Recommended Cameras for Humanoids"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Camera"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Resolution"}),(0,i.jsx)(n.th,{children:"FPS"}),(0,i.jsx)(n.th,{children:"Price"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Intel RealSense D435i"})}),(0,i.jsx)(n.td,{children:"Depth + RGB + IMU"}),(0,i.jsx)(n.td,{children:"1920x1080"}),(0,i.jsx)(n.td,{children:"90"}),(0,i.jsx)(n.td,{children:"$350"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Intel RealSense D455"})}),(0,i.jsx)(n.td,{children:"Depth + RGB"}),(0,i.jsx)(n.td,{children:"1920x1080"}),(0,i.jsx)(n.td,{children:"90"}),(0,i.jsx)(n.td,{children:"$400"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ZED 2"})}),(0,i.jsx)(n.td,{children:"Stereo"}),(0,i.jsx)(n.td,{children:"2208x1242"}),(0,i.jsx)(n.td,{children:"60"}),(0,i.jsx)(n.td,{children:"$450"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Azure Kinect"})}),(0,i.jsx)(n.td,{children:"ToF + RGB"}),(0,i.jsx)(n.td,{children:"3840x2160"}),(0,i.jsx)(n.td,{children:"30"}),(0,i.jsx)(n.td,{children:"$400"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"3-imu-inertial-measurement-unit",children:"3. IMU (Inertial Measurement Unit)"}),"\n",(0,i.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,i.jsx)(n.p,{children:"An IMU combines three sensors:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accelerometer:"})," Measures linear acceleration (3-axis)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gyroscope:"})," Measures angular velocity (3-axis)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Magnetometer:"})," Measures magnetic field (3-axis, optional)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-format-1",children:"Data Format"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# ROS 2 sensor_msgs/Imu\r\n{\r\n    'header': {\r\n        'stamp': timestamp,\r\n        'frame_id': 'imu_link'\r\n    },\r\n    'orientation': {  # Quaternion (if computed)\r\n        'x': 0.0, 'y': 0.0, 'z': 0.0, 'w': 1.0\r\n    },\r\n    'angular_velocity': {  # rad/s\r\n        'x': 0.01, 'y': -0.02, 'z': 0.0\r\n    },\r\n    'linear_acceleration': {  # m/s\xb2\r\n        'x': 0.1, 'y': 0.2, 'z': 9.81\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"orientation-estimation",children:"Orientation Estimation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation\r\n\r\nclass IMUProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('imu_processor')\r\n        \r\n        self.subscription = self.create_subscription(\r\n            Imu,\r\n            '/imu/data',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n        \r\n        # Complementary filter state\r\n        self.pitch = 0.0\r\n        self.roll = 0.0\r\n        self.alpha = 0.98  # Filter coefficient\r\n        self.dt = 0.01  # 100 Hz\r\n    \r\n    def imu_callback(self, msg):\r\n        \"\"\"Process IMU data\"\"\"\r\n        # Extract data\r\n        ax = msg.linear_acceleration.x\r\n        ay = msg.linear_acceleration.y\r\n        az = msg.linear_acceleration.z\r\n        \r\n        gx = msg.angular_velocity.x\r\n        gy = msg.angular_velocity.y\r\n        \r\n        # Accelerometer-based angle (noisy but no drift)\r\n        accel_pitch = np.arctan2(ay, np.sqrt(ax**2 + az**2))\r\n        accel_roll = np.arctan2(-ax, az)\r\n        \r\n        # Gyroscope integration (drifts but smooth)\r\n        gyro_pitch = self.pitch + gx * self.dt\r\n        gyro_roll = self.roll + gy * self.dt\r\n        \r\n        # Complementary filter (combine both)\r\n        self.pitch = self.alpha * gyro_pitch + (1 - self.alpha) * accel_pitch\r\n        self.roll = self.alpha * gyro_roll + (1 - self.alpha) * accel_roll\r\n        \r\n        # Check for dangerous tilt\r\n        if abs(self.pitch) > 0.5 or abs(self.roll) > 0.5:\r\n            self.get_logger().warn(\r\n                f'High tilt detected! Pitch: {self.pitch:.2f}, Roll: {self.roll:.2f}'\r\n            )\n"})}),"\n",(0,i.jsx)(n.h3,{id:"imu-calibration",children:"IMU Calibration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def calibrate_imu(self, num_samples=1000):\r\n    """Calibrate IMU by collecting stationary data"""\r\n    gyro_samples = []\r\n    accel_samples = []\r\n    \r\n    for i in range(num_samples):\r\n        # Collect data while robot is stationary\r\n        gyro_samples.append([gx, gy, gz])\r\n        accel_samples.append([ax, ay, az])\r\n    \r\n    # Calculate biases\r\n    gyro_bias = np.mean(gyro_samples, axis=0)\r\n    accel_bias = np.mean(accel_samples, axis=0)\r\n    accel_bias[2] -= 9.81  # Remove gravity from Z\r\n    \r\n    return gyro_bias, accel_bias\n'})}),"\n",(0,i.jsx)(n.h3,{id:"recommended-imus",children:"Recommended IMUs"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"IMU"}),(0,i.jsx)(n.th,{children:"DOF"}),(0,i.jsx)(n.th,{children:"Rate"}),(0,i.jsx)(n.th,{children:"Price"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"BNO055"})}),(0,i.jsx)(n.td,{children:"9-DOF"}),(0,i.jsx)(n.td,{children:"100 Hz"}),(0,i.jsx)(n.td,{children:"$30"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"MPU-9250"})}),(0,i.jsx)(n.td,{children:"9-DOF"}),(0,i.jsx)(n.td,{children:"1000 Hz"}),(0,i.jsx)(n.td,{children:"$15"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"VectorNav VN-100"})}),(0,i.jsx)(n.td,{children:"9-DOF"}),(0,i.jsx)(n.td,{children:"800 Hz"}),(0,i.jsx)(n.td,{children:"$500"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Xsens MTi-30"})}),(0,i.jsx)(n.td,{children:"9-DOF"}),(0,i.jsx)(n.td,{children:"2000 Hz"}),(0,i.jsx)(n.td,{children:"$2,000"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"4-forcetorque-sensors",children:"4. Force/Torque Sensors"}),"\n",(0,i.jsx)(n.h3,{id:"purpose",children:"Purpose"}),"\n",(0,i.jsx)(n.p,{children:"Measure forces and torques at contact points:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feet:"})," Ground reaction forces for balance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hands:"})," Grasp force control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Joints:"})," Collision detection"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-format-2",children:"Data Format"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# ROS 2 geometry_msgs/WrenchStamped\r\n{\r\n    'header': {...},\r\n    'wrench': {\r\n        'force': {  # Newtons\r\n            'x': 10.5, 'y': -2.3, 'z': 150.0\r\n        },\r\n        'torque': {  # Newton-meters\r\n            'x': 0.5, 'y': 0.3, 'z': -0.1\r\n        }\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"integration-example-1",children:"Integration Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import WrenchStamped\r\nimport numpy as np\r\n\r\nclass ForceProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('force_processor')\r\n        \r\n        # Subscribe to force sensors in feet\r\n        self.left_foot_sub = self.create_subscription(\r\n            WrenchStamped,\r\n            '/left_foot/force',\r\n            lambda msg: self.force_callback(msg, 'left'),\r\n            10\r\n        )\r\n        \r\n        self.right_foot_sub = self.create_subscription(\r\n            WrenchStamped,\r\n            '/right_foot/force',\r\n            lambda msg: self.force_callback(msg, 'right'),\r\n            10\r\n        )\r\n        \r\n        self.left_force = np.zeros(3)\r\n        self.right_force = np.zeros(3)\r\n    \r\n    def force_callback(self, msg, foot):\r\n        \"\"\"Process force sensor data\"\"\"\r\n        force = np.array([\r\n            msg.wrench.force.x,\r\n            msg.wrench.force.y,\r\n            msg.wrench.force.z\r\n        ])\r\n        \r\n        if foot == 'left':\r\n            self.left_force = force\r\n        else:\r\n            self.right_force = force\r\n        \r\n        # Calculate Center of Pressure (CoP)\r\n        total_force_z = self.left_force[2] + self.right_force[2]\r\n        \r\n        if total_force_z > 10:  # Robot is standing\r\n            # Check balance\r\n            force_imbalance = abs(self.left_force[2] - self.right_force[2])\r\n            \r\n            if force_imbalance > 100:  # Newtons\r\n                self.get_logger().warn(\r\n                    f'Force imbalance detected: {force_imbalance:.1f}N'\r\n                )\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Combining multiple sensors for robust perception:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph LR\r\n    IMU[IMU] --\x3e FILTER[Extended Kalman Filter]\r\n    ENCODER[Encoders] --\x3e FILTER\r\n    LIDAR[LiDAR] --\x3e FILTER\r\n    CAMERA[Camera] --\x3e FILTER\r\n    FILTER --\x3e STATE[Robot State Estimate]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"example-imu--encoder-fusion",children:"Example: IMU + Encoder Fusion"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SensorFusion(Node):\r\n    def __init__(self):\r\n        super().__init__(\'sensor_fusion\')\r\n        \r\n        # State: [x, y, theta, vx, vy, omega]\r\n        self.state = np.zeros(6)\r\n        self.covariance = np.eye(6)\r\n        \r\n        # Subscribe to sensors\r\n        self.imu_sub = self.create_subscription(Imu, \'/imu/data\', self.imu_callback, 10)\r\n        self.odom_sub = self.create_subscription(Odometry, \'/odom\', self.odom_callback, 10)\r\n        \r\n        # Publish fused estimate\r\n        self.state_pub = self.create_publisher(Odometry, \'/fused_odom\', 10)\r\n    \r\n    def imu_callback(self, msg):\r\n        """Update with IMU measurement"""\r\n        # Extract angular velocity\r\n        omega_z = msg.angular_velocity.z\r\n        \r\n        # Kalman filter update\r\n        # ...\r\n    \r\n    def odom_callback(self, msg):\r\n        """Update with encoder odometry"""\r\n        # Extract position and velocity\r\n        # ...\r\n        \r\n        # Kalman filter update\r\n        # ...\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR:"})," 3D mapping and obstacle detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cameras:"})," Visual perception and object recognition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU:"})," Orientation and balance control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force/Torque:"})," Contact detection and compliant control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Fusion:"})," Combines multiple sensors for robust state estimation"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Proper sensor integration is critical for humanoid robot perception and control."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/",children:"ROS 2 Sensor Messages"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://dev.intelrealsense.com/",children:"Intel RealSense Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://ahrs.readthedocs.io/",children:"IMU Sensor Fusion"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/TixiaoShan/LIO-SAM",children:"LiDAR SLAM"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>l});var s=r(6540);const i={},t=s.createContext(i);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);