"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[1144],{720:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"Vision-Language-Action/module-05-intro","title":"Vision-Language-Action (VLA)","description":"Module Overview","source":"@site/docs/05-Vision-Language-Action/index.md","sourceDirName":"05-Vision-Language-Action","slug":"/module-05","permalink":"/physical-ai-hackathon/docs/module-05","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/05-Vision-Language-Action/index.md","tags":[],"version":"current","frontMatter":{"id":"module-05-intro","title":"Vision-Language-Action (VLA)","slug":"/module-05"},"sidebar":"tutorialSidebar","previous":{"title":"Deploying to Jetson Hardware","permalink":"/physical-ai-hackathon/docs/module-04/jetson-deployment"},"next":{"title":"Voice Recognition and Natural Language Processing","permalink":"/physical-ai-hackathon/docs/module-05/voice-to-action"}}');var t=i(4848),s=i(8453);const a={id:"module-05-intro",title:"Vision-Language-Action (VLA)",slug:"/module-05"},l="Vision-Language-Action (VLA)",r={},c=[{value:"Module Overview",id:"module-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Module Structure",id:"module-structure",level:3},{value:"Prerequisites",id:"prerequisites",level:3}];function u(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,t.jsx)(n.p,{children:"This capstone module focuses on the convergence of vision, language, and action in robotics. Students will learn to integrate GPT models for conversational robotics, use OpenAI Whisper for voice commands, and apply cognitive planning to translate natural language into sequences of ROS 2 actions. The module culminates in the Autonomous Humanoid capstone project."}),"\n",(0,t.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate GPT models for conversational robotics"}),"\n",(0,t.jsx)(n.li,{children:"Implement voice recognition using OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Apply cognitive planning for natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Create end-to-end Vision-Language-Action systems"}),"\n",(0,t.jsx)(n.li,{children:"Execute a complete humanoid robot capstone project"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(n.p,{children:"This module contains the following topics:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice recognition and natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Cognitive planning for robot action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Multi-modal integration (Vision-Language-Action)"}),"\n",(0,t.jsx)(n.li,{children:"Conversational robotics implementation"}),"\n",(0,t.jsx)(n.li,{children:"Autonomous humanoid capstone project"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Students should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Proficiency in ROS 2 and simulation systems"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of deep learning models"}),"\n",(0,t.jsx)(n.li,{children:"Experience with natural language processing"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);