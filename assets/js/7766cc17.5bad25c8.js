"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[7343],{2676:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Vision-Language-Action/VLA-Implementation/end-to-end-vla","title":"End-to-End VLA Implementation","description":"Introduction","source":"@site/docs/05-Vision-Language-Action/05-VLA-Implementation/index.md","sourceDirName":"05-Vision-Language-Action/05-VLA-Implementation","slug":"/module-05/end-to-end-vla","permalink":"/physical-ai-hackathon/docs/module-05/end-to-end-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/05-Vision-Language-Action/05-VLA-Implementation/index.md","tags":[],"version":"current","frontMatter":{"id":"end-to-end-vla","title":"End-to-End VLA Implementation","slug":"/module-05/end-to-end-vla"},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Modal Interaction","permalink":"/physical-ai-hackathon/docs/module-05/multi-modal-interaction"},"next":{"title":"Humanoid Capstone Project","permalink":"/physical-ai-hackathon/docs/module-05/capstone-project"}}');var s=r(4848),o=r(8453);const a={id:"end-to-end-vla",title:"End-to-End VLA Implementation",slug:"/module-05/end-to-end-vla"},i="End-to-End VLA Implementation",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Complete VLA Architecture",id:"complete-vla-architecture",level:2},{value:"Complete VLA System Implementation",id:"complete-vla-system-implementation",level:2},{value:"Example Interaction",id:"example-interaction",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"end-to-end-vla-implementation",children:"End-to-End VLA Implementation"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(e.p,{children:["This chapter brings together ",(0,s.jsx)(e.strong,{children:"Vision"}),", ",(0,s.jsx)(e.strong,{children:"Language"}),", and ",(0,s.jsx)(e.strong,{children:"Action"})," into a complete system. You'll build an end-to-end VLA pipeline that enables a humanoid robot to understand visual scenes, process natural language commands, and execute physical actions."]}),"\n",(0,s.jsx)(e.h2,{id:"complete-vla-architecture",children:"Complete VLA Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Perception"\r\n        CAM[Camera Feed]\r\n        MIC[Microphone]\r\n        SENSORS[Robot Sensors]\r\n    end\r\n    \r\n    subgraph "Vision Processing"\r\n        DETECT[Object Detection]\r\n        SEGMENT[Segmentation]\r\n        DEPTH[Depth Estimation]\r\n    end\r\n    \r\n    subgraph "Language Processing"\r\n        ASR[Speech-to-Text]\r\n        NLU[Language Understanding]\r\n        LLM[LLM Reasoning]\r\n    end\r\n    \r\n    subgraph "Action Planning"\r\n        TASK_PLAN[Task Planner]\r\n        MOTION_PLAN[Motion Planner]\r\n        GRASP_PLAN[Grasp Planner]\r\n    end\r\n    \r\n    subgraph "Execution"\r\n        NAV[Navigation]\r\n        MANIP[Manipulation]\r\n        SPEECH[Speech Output]\r\n    end\r\n    \r\n    CAM --\x3e DETECT\r\n    CAM --\x3e SEGMENT\r\n    CAM --\x3e DEPTH\r\n    MIC --\x3e ASR\r\n    \r\n    DETECT --\x3e LLM\r\n    SEGMENT --\x3e LLM\r\n    DEPTH --\x3e LLM\r\n    ASR --\x3e NLU\r\n    NLU --\x3e LLM\r\n    SENSORS --\x3e LLM\r\n    \r\n    LLM --\x3e TASK_PLAN\r\n    TASK_PLAN --\x3e MOTION_PLAN\r\n    TASK_PLAN --\x3e GRASP_PLAN\r\n    \r\n    MOTION_PLAN --\x3e NAV\r\n    GRASP_PLAN --\x3e MANIP\r\n    LLM --\x3e SPEECH\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"complete-vla-system-implementation",children:"Complete VLA System Implementation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nComplete end-to-end VLA system for humanoid robots.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom sensor_msgs.msg import Image, JointState\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport openai\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nimport whisper\r\n\r\nclass EndToEndVLA(Node):\r\n    """\r\n    Complete Vision-Language-Action system.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'end_to_end_vla\')\r\n        \r\n        # Initialize AI models\r\n        self.setup_models()\r\n        \r\n        # Scene understanding\r\n        self.scene_graph = {}\r\n        self.detected_objects = []\r\n        \r\n        # Task state\r\n        self.current_task = None\r\n        self.task_queue = []\r\n        \r\n        # ROS interfaces\r\n        self.setup_ros_interfaces()\r\n        \r\n        self.get_logger().info(\'End-to-End VLA System initialized\')\r\n    \r\n    def setup_models(self):\r\n        """Initialize all AI models"""\r\n        # Vision: YOLO for object detection\r\n        self.yolo_model = YOLO(\'yolov8n.pt\')\r\n        \r\n        # Language: Whisper for speech recognition\r\n        self.whisper_model = whisper.load_model(\'base\')\r\n        \r\n        # Reasoning: GPT-4 for task planning\r\n        self.llm_client = openai.Client(api_key=os.getenv(\'OPENAI_API_KEY\'))\r\n        \r\n        self.get_logger().info(\'AI models loaded\')\r\n    \r\n    def setup_ros_interfaces(self):\r\n        """Setup ROS publishers, subscribers, action clients"""\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10\r\n        )\r\n        \r\n        self.depth_sub = self.create_subscription(\r\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\r\n        )\r\n        \r\n        self.audio_sub = self.create_subscription(\r\n            String, \'/voice_command\', self.voice_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.speech_pub = self.create_publisher(String, \'/robot/speech\', 10)\r\n        self.status_pub = self.create_publisher(String, \'/robot/status\', 10)\r\n        \r\n        # Action clients\r\n        self.nav_client = ActionClient(self, NavigateToPoint, \'navigate\')\r\n        self.grasp_client = ActionClient(self, GraspObject, \'grasp\')\r\n        \r\n        # CV Bridge\r\n        self.bridge = CvBridge()\r\n        \r\n        # State\r\n        self.current_rgb = None\r\n        self.current_depth = None\r\n    \r\n    def image_callback(self, msg):\r\n        """Process RGB image"""\r\n        self.current_rgb = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n        \r\n        # Run object detection\r\n        self.detect_objects()\r\n    \r\n    def depth_callback(self, msg):\r\n        """Process depth image"""\r\n        self.current_depth = self.bridge.imgmsg_to_cv2(msg, \'passthrough\')\r\n    \r\n    def voice_callback(self, msg):\r\n        """Process voice command"""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Received command: {command}\')\r\n        \r\n        # Process VLA pipeline\r\n        self.process_vla_command(command)\r\n    \r\n    def detect_objects(self):\r\n        """Detect objects in current scene"""\r\n        if self.current_rgb is None:\r\n            return\r\n        \r\n        # Run YOLO\r\n        results = self.yolo_model(self.current_rgb)\r\n        \r\n        # Extract detections\r\n        self.detected_objects = []\r\n        \r\n        for r in results:\r\n            boxes = r.boxes\r\n            for box in boxes:\r\n                # Get box coordinates\r\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\r\n                \r\n                # Get class and confidence\r\n                cls = int(box.cls[0])\r\n                conf = float(box.conf[0])\r\n                label = self.yolo_model.names[cls]\r\n                \r\n                # Get 3D position from depth\r\n                center_x = int((x1 + x2) / 2)\r\n                center_y = int((y1 + y2) / 2)\r\n                \r\n                if self.current_depth is not None:\r\n                    depth = self.current_depth[center_y, center_x]\r\n                    position_3d = self.pixel_to_3d(center_x, center_y, depth)\r\n                else:\r\n                    position_3d = None\r\n                \r\n                self.detected_objects.append({\r\n                    \'label\': label,\r\n                    \'confidence\': conf,\r\n                    \'bbox\': (x1, y1, x2, y2),\r\n                    \'position_3d\': position_3d\r\n                })\r\n    \r\n    def pixel_to_3d(self, u, v, depth):\r\n        """Convert pixel coordinates to 3D position"""\r\n        # Camera intrinsics (example values)\r\n        fx, fy = 525.0, 525.0\r\n        cx, cy = 319.5, 239.5\r\n        \r\n        # Calculate 3D position\r\n        z = depth / 1000.0  # Convert mm to meters\r\n        x = (u - cx) * z / fx\r\n        y = (v - cy) * z / fy\r\n        \r\n        return (x, y, z)\r\n    \r\n    def process_vla_command(self, command):\r\n        """\r\n        Process complete VLA pipeline.\r\n        \r\n        Args:\r\n            command: Natural language command\r\n        """\r\n        self.speak("I\'m processing your request...")\r\n        \r\n        # 1. Build scene understanding\r\n        scene_description = self.build_scene_description()\r\n        \r\n        # 2. Use LLM to plan task\r\n        task_plan = self.plan_task_with_llm(command, scene_description)\r\n        \r\n        # 3. Execute task plan\r\n        self.execute_task_plan(task_plan)\r\n    \r\n    def build_scene_description(self):\r\n        """\r\n        Build textual description of current scene.\r\n        \r\n        Returns:\r\n            str: Scene description\r\n        """\r\n        if not self.detected_objects:\r\n            return "I don\'t see any objects in the scene."\r\n        \r\n        # Group objects by type\r\n        object_counts = {}\r\n        for obj in self.detected_objects:\r\n            label = obj[\'label\']\r\n            object_counts[label] = object_counts.get(label, 0) + 1\r\n        \r\n        # Build description\r\n        description = "I can see: "\r\n        items = []\r\n        for label, count in object_counts.items():\r\n            if count == 1:\r\n                items.append(f"a {label}")\r\n            else:\r\n                items.append(f"{count} {label}s")\r\n        \r\n        description += ", ".join(items)\r\n        \r\n        return description\r\n    \r\n    def plan_task_with_llm(self, command, scene_description):\r\n        """\r\n        Use LLM to plan task execution.\r\n        \r\n        Args:\r\n            command: User command\r\n            scene_description: Current scene\r\n            \r\n        Returns:\r\n            dict: Task plan\r\n        """\r\n        prompt = f"""\r\nYou are a humanoid robot. Plan how to execute the user\'s command.\r\n\r\nUser command: "{command}"\r\nCurrent scene: {scene_description}\r\n\r\nAvailable actions:\r\n- navigate(x, y, z): Move to position\r\n- grasp(object_name): Pick up object\r\n- place(x, y, z): Place held object\r\n- speak(text): Say something\r\n\r\nDetected objects with 3D positions:\r\n{self.format_detected_objects()}\r\n\r\nGenerate a step-by-step plan in JSON format:\r\n{{\r\n  "task_summary": "brief description",\r\n  "steps": [\r\n    {{"action": "action_name", "parameters": {{...}}, "reasoning": "why"}}\r\n  ]\r\n}}\r\n"""\r\n        \r\n        response = self.llm_client.chat.completions.create(\r\n            model=\'gpt-4\',\r\n            messages=[\r\n                {\'role\': \'system\', \'content\': \'You are a helpful robot assistant.\'},\r\n                {\'role\': \'user\', \'content\': prompt}\r\n            ],\r\n            response_format={\'type\': \'json_object\'}\r\n        )\r\n        \r\n        import json\r\n        plan = json.loads(response.choices[0].message.content)\r\n        \r\n        self.get_logger().info(f\'Generated plan: {plan["task_summary"]}\')\r\n        \r\n        return plan\r\n    \r\n    def format_detected_objects(self):\r\n        """Format detected objects for LLM"""\r\n        formatted = []\r\n        for obj in self.detected_objects:\r\n            if obj[\'position_3d\']:\r\n                x, y, z = obj[\'position_3d\']\r\n                formatted.append(\r\n                    f"- {obj[\'label\']} at position ({x:.2f}, {y:.2f}, {z:.2f}m)"\r\n                )\r\n        return \'\\n\'.join(formatted)\r\n    \r\n    def execute_task_plan(self, plan):\r\n        """\r\n        Execute the generated task plan.\r\n        \r\n        Args:\r\n            plan: Task plan from LLM\r\n        """\r\n        self.speak(f"I will {plan[\'task_summary\']}")\r\n        \r\n        for step in plan[\'steps\']:\r\n            action = step[\'action\']\r\n            params = step[\'parameters\']\r\n            \r\n            self.get_logger().info(f"Executing: {action} with {params}")\r\n            \r\n            # Execute action\r\n            if action == \'navigate\':\r\n                self.execute_navigate(params)\r\n            elif action == \'grasp\':\r\n                self.execute_grasp(params)\r\n            elif action == \'place\':\r\n                self.execute_place(params)\r\n            elif action == \'speak\':\r\n                self.speak(params[\'text\'])\r\n            \r\n            # Brief pause between steps\r\n            time.sleep(0.5)\r\n        \r\n        self.speak("Task completed!")\r\n    \r\n    def execute_navigate(self, params):\r\n        """Execute navigation action"""\r\n        goal = NavigateToPoint.Goal()\r\n        goal.target_x = params[\'x\']\r\n        goal.target_y = params[\'y\']\r\n        goal.target_z = params.get(\'z\', 0.0)\r\n        \r\n        self.get_logger().info(f\'Navigating to ({goal.target_x}, {goal.target_y})\')\r\n        \r\n        future = self.nav_client.send_goal_async(goal)\r\n        rclpy.spin_until_future_complete(self, future)\r\n        \r\n        goal_handle = future.result()\r\n        if goal_handle.accepted:\r\n            result_future = goal_handle.get_result_async()\r\n            rclpy.spin_until_future_complete(self, result_future)\r\n    \r\n    def execute_grasp(self, params):\r\n        """Execute grasp action"""\r\n        object_name = params[\'object_name\']\r\n        \r\n        # Find object in detected objects\r\n        target_obj = None\r\n        for obj in self.detected_objects:\r\n            if obj[\'label\'] == object_name:\r\n                target_obj = obj\r\n                break\r\n        \r\n        if not target_obj:\r\n            self.speak(f"I cannot find the {object_name}")\r\n            return\r\n        \r\n        # Send grasp goal\r\n        goal = GraspObject.Goal()\r\n        goal.object_name = object_name\r\n        \r\n        if target_obj[\'position_3d\']:\r\n            x, y, z = target_obj[\'position_3d\']\r\n            goal.target_pose.position.x = x\r\n            goal.target_pose.position.y = y\r\n            goal.target_pose.position.z = z\r\n        \r\n        self.get_logger().info(f\'Grasping {object_name}\')\r\n        \r\n        future = self.grasp_client.send_goal_async(goal)\r\n        rclpy.spin_until_future_complete(self, future)\r\n    \r\n    def execute_place(self, params):\r\n        """Execute place action"""\r\n        self.get_logger().info(f\'Placing object at ({params["x"]}, {params["y"]})\')\r\n        # Implementation...\r\n    \r\n    def speak(self, text):\r\n        """Make robot speak"""\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_pub.publish(msg)\r\n        \r\n        self.get_logger().info(f\'Robot says: {text}\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vla = EndToEndVLA()\r\n    \r\n    try:\r\n        rclpy.spin(vla)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        vla.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"example-interaction",children:"Example Interaction"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"User:"}),' "Bring me the cup on the table"']}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"VLA Processing:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision:"})," Detects cup at (1.2, 0.5, 0.8m), table at (1.0, 0.0, 0.0m)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language:"}),' Understands "bring" = navigate + grasp + navigate + place']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Plan:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-json",children:'{\r\n  "task_summary": "bring the cup to the user",\r\n  "steps": [\r\n    {"action": "navigate", "parameters": {"x": 1.2, "y": 0.5}},\r\n    {"action": "grasp", "parameters": {"object_name": "cup"}},\r\n    {"action": "navigate", "parameters": {"x": 0.0, "y": 0.0}},\r\n    {"action": "place", "parameters": {"x": 0.0, "y": 0.5}}\r\n  ]\r\n}\n'})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Execution:"})," Robot navigates, grasps cup, returns, places cup"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class OptimizedVLA(EndToEndVLA):\r\n    """\r\n    Optimized VLA with caching and parallel processing.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n        # Caching\r\n        self.scene_cache = None\r\n        self.cache_timestamp = None\r\n        \r\n        # Parallel processing\r\n        from concurrent.futures import ThreadPoolExecutor\r\n        self.executor = ThreadPoolExecutor(max_workers=3)\r\n    \r\n    def process_vla_command_optimized(self, command):\r\n        """Optimized VLA processing with parallelization"""\r\n        # Run vision and language processing in parallel\r\n        vision_future = self.executor.submit(self.build_scene_description)\r\n        \r\n        # Wait for vision\r\n        scene_description = vision_future.result()\r\n        \r\n        # Plan and execute\r\n        task_plan = self.plan_task_with_llm(command, scene_description)\r\n        self.execute_task_plan(task_plan)\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"End-to-end VLA"})," integrates vision, language, and action"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"YOLO"})," for object detection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Whisper"})," for speech recognition"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPT-4"})," for task planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ROS 2 actions"})," for execution"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"This complete system demonstrates the power of combining modern AI with robotics to create truly intelligent, interactive humanoid robots."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://robotics-transformer2.github.io/",children:"RT-2: Vision-Language-Action Models"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://palm-e.github.io/",children:"PaLM-E: Embodied Multimodal Language Model"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://say-can.github.io/",children:"SayCan: Grounding Language in Robotic Affordances"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://openai.com/research/clip",children:"CLIP: Connecting Text and Images"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>i});var t=r(6540);const s={},o=t.createContext(s);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);