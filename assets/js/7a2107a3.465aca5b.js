"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[3043],{5155:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Vision-Language-Action/Cognitive-Planning-LLMs/cognitive-planning-llms","title":"Cognitive Planning with LLMs","description":"Introduction","source":"@site/docs/05-Vision-Language-Action/03-Cognitive-Planning-LLMs/index.md","sourceDirName":"05-Vision-Language-Action/03-Cognitive-Planning-LLMs","slug":"/module-05/cognitive-planning-llms","permalink":"/physical-ai-hackathon/docs/module-05/cognitive-planning-llms","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/05-Vision-Language-Action/03-Cognitive-Planning-LLMs/index.md","tags":[],"version":"current","frontMatter":{"id":"cognitive-planning-llms","title":"Cognitive Planning with LLMs","slug":"/module-05/cognitive-planning-llms"},"sidebar":"tutorialSidebar","previous":{"title":"Voice Recognition and Natural Language Processing","permalink":"/physical-ai-hackathon/docs/module-05/voice-to-action"},"next":{"title":"Multi-Modal Interaction","permalink":"/physical-ai-hackathon/docs/module-05/multi-modal-interaction"}}');var i=r(4848),a=r(8453);const s={id:"cognitive-planning-llms",title:"Cognitive Planning with LLMs",slug:"/module-05/cognitive-planning-llms"},l="Cognitive Planning with LLMs",o={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Why LLMs for Robot Planning?",id:"why-llms-for-robot-planning",level:2},{value:"Traditional Approach:",id:"traditional-approach",level:3},{value:"LLM Approach:",id:"llm-approach",level:3},{value:"Complete LLM Planning System",id:"complete-llm-planning-system",level:2},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Key Principles",id:"key-principles",level:3},{value:"Example Prompts",id:"example-prompts",level:3},{value:"Advanced: Multi-Turn Planning",id:"advanced-multi-turn-planning",level:2},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"\u2705 DO:",id:"-do",level:3},{value:"\u274c DON&#39;T:",id:"-dont",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Large Language Models (LLMs)"})," like GPT-4 can transform natural language commands into structured robot action sequences. This chapter shows how to use LLMs for ",(0,i.jsx)(e.strong,{children:"cognitive planning"}),' - translating high-level human intent ("Clean the room") into executable robot behaviors using ROS 2 actions.']}),"\n",(0,i.jsx)(e.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Human Interface"\r\n        VOICE[Voice Input]\r\n        TEXT[Text Input]\r\n    end\r\n    \r\n    subgraph "LLM Planning Layer"\r\n        LLM[GPT-4 / Claude]\r\n        PROMPT[Prompt Engineering]\r\n        PARSER[Response Parser]\r\n    end\r\n    \r\n    subgraph "Task Decomposition"\r\n        DECOMP[Task Decomposer]\r\n        VALIDATOR[Action Validator]\r\n        SEQUENCER[Action Sequencer]\r\n    end\r\n    \r\n    subgraph "ROS 2 Execution"\r\n        ACTIONS[Action Servers]\r\n        SERVICES[Service Servers]\r\n        TOPICS[Topic Publishers]\r\n    end\r\n    \r\n    subgraph "Robot Hardware"\r\n        NAV[Navigation]\r\n        MANIP[Manipulation]\r\n        PERCEPTION[Perception]\r\n    end\r\n    \r\n    VOICE --\x3e LLM\r\n    TEXT --\x3e LLM\r\n    LLM --\x3e PROMPT\r\n    PROMPT --\x3e PARSER\r\n    PARSER --\x3e DECOMP\r\n    DECOMP --\x3e VALIDATOR\r\n    VALIDATOR --\x3e SEQUENCER\r\n    SEQUENCER --\x3e ACTIONS\r\n    SEQUENCER --\x3e SERVICES\r\n    ACTIONS --\x3e NAV\r\n    ACTIONS --\x3e MANIP\r\n    SERVICES --\x3e PERCEPTION\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"why-llms-for-robot-planning",children:"Why LLMs for Robot Planning?"}),"\n",(0,i.jsx)(e.h3,{id:"traditional-approach",children:"Traditional Approach:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Hardcoded task logic\r\nif command == "clean room":\r\n    navigate_to("room")\r\n    detect_objects()\r\n    for obj in objects:\r\n        if obj.type == "trash":\r\n            pick(obj)\r\n            navigate_to("bin")\r\n            place(obj)\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Problems:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Brittle to variations"}),"\n",(0,i.jsx)(e.li,{children:"Requires programming for every task"}),"\n",(0,i.jsx)(e.li,{children:"No generalization"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"llm-approach",children:"LLM Approach:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Flexible, natural language understanding\r\ncommand = "Clean the room and organize the books"\r\nplan = llm.generate_plan(command, context=robot_state)\r\nexecute_plan(plan)\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Benefits:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Handles variations naturally"}),"\n",(0,i.jsx)(e.li,{children:"Generalizes to new tasks"}),"\n",(0,i.jsx)(e.li,{children:"Understands context and intent"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"complete-llm-planning-system",children:"Complete LLM Planning System"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLLM-based cognitive planner for humanoid robots\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport openai\r\nimport json\r\nfrom typing import List, Dict, Any\r\n\r\nclass CognitivePlanner(Node):\r\n    """\r\n    Uses LLM to convert natural language to robot action sequences.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'cognitive_planner\')\r\n        \r\n        # Parameters\r\n        self.declare_parameter(\'openai_api_key\', \'\')\r\n        self.declare_parameter(\'model\', \'gpt-4-turbo-preview\')\r\n        self.declare_parameter(\'temperature\', 0.3)\r\n        \r\n        api_key = self.get_parameter(\'openai_api_key\').value\r\n        self.model = self.get_parameter(\'model\').value\r\n        self.temperature = self.get_parameter(\'temperature\').value\r\n        \r\n        # Initialize OpenAI client\r\n        self.client = openai.Client(api_key=api_key)\r\n        \r\n        # Robot capabilities (for LLM context)\r\n        self.capabilities = {\r\n            "actions": [\r\n                {\r\n                    "name": "navigate",\r\n                    "description": "Move to a location",\r\n                    "parameters": {"x": "float", "y": "float", "theta": "float"}\r\n                },\r\n                {\r\n                    "name": "pick",\r\n                    "description": "Pick up an object",\r\n                    "parameters": {"object_name": "string"}\r\n                },\r\n                {\r\n                    "name": "place",\r\n                    "description": "Place held object at location",\r\n                    "parameters": {"x": "float", "y": "float"}\r\n                },\r\n                {\r\n                    "name": "scan",\r\n                    "description": "Scan environment for objects",\r\n                    "parameters": {}\r\n                },\r\n                {\r\n                    "name": "wait",\r\n                    "description": "Wait for specified duration",\r\n                    "parameters": {"duration": "float"}\r\n                }\r\n            ],\r\n            "known_locations": {\r\n                "kitchen": {"x": 5.0, "y": 2.0},\r\n                "living_room": {"x": 0.0, "y": 0.0},\r\n                "bedroom": {"x": -3.0, "y": 4.0},\r\n                "charging_station": {"x": 0.0, "y": -2.0}\r\n            }\r\n        }\r\n        \r\n        # Subscribe to commands\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'task_command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        # Publish plan status\r\n        self.status_pub = self.create_publisher(\r\n            String,\r\n            \'plan_status\',\r\n            10\r\n        )\r\n        \r\n        # Action clients\r\n        self.setup_action_clients()\r\n        \r\n        self.get_logger().info(\'Cognitive Planner initialized\')\r\n    \r\n    def setup_action_clients(self):\r\n        """Setup action clients for robot control"""\r\n        # Navigation action client\r\n        self.nav_client = ActionClient(\r\n            self,\r\n            NavigateToPoint,\r\n            \'navigate_to_point\'\r\n        )\r\n        \r\n        # Manipulation action client\r\n        self.manip_client = ActionClient(\r\n            self,\r\n            PickPlace,\r\n            \'pick_place\'\r\n        )\r\n    \r\n    def command_callback(self, msg):\r\n        """Process incoming task command"""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Received command: "{command}"\')\r\n        \r\n        # Generate plan using LLM\r\n        plan = self.generate_plan(command)\r\n        \r\n        if plan:\r\n            self.get_logger().info(f\'Generated plan with {len(plan["steps"])} steps\')\r\n            self.execute_plan(plan)\r\n        else:\r\n            self.get_logger().error(\'Failed to generate valid plan\')\r\n    \r\n    def generate_plan(self, command: str) -> Dict[str, Any]:\r\n        """\r\n        Use LLM to generate action plan from natural language.\r\n        \r\n        Args:\r\n            command: Natural language task description\r\n            \r\n        Returns:\r\n            dict: Structured action plan\r\n        """\r\n        # Construct system prompt\r\n        system_prompt = self.build_system_prompt()\r\n        \r\n        # Construct user prompt with context\r\n        user_prompt = f"""\r\nTask: {command}\r\n\r\nCurrent robot state:\r\n- Location: living_room\r\n- Battery: 85%\r\n- Holding: nothing\r\n\r\nGenerate a step-by-step plan to complete this task.\r\n"""\r\n        \r\n        try:\r\n            response = self.client.chat.completions.create(\r\n                model=self.model,\r\n                messages=[\r\n                    {"role": "system", "content": system_prompt},\r\n                    {"role": "user", "content": user_prompt}\r\n                ],\r\n                temperature=self.temperature,\r\n                response_format={"type": "json_object"}\r\n            )\r\n            \r\n            # Parse response\r\n            plan_json = response.choices[0].message.content\r\n            plan = json.loads(plan_json)\r\n            \r\n            # Validate plan\r\n            if self.validate_plan(plan):\r\n                return plan\r\n            else:\r\n                self.get_logger().error(\'Generated plan failed validation\')\r\n                return None\r\n                \r\n        except Exception as e:\r\n            self.get_logger().error(f\'LLM error: {str(e)}\')\r\n            return None\r\n    \r\n    def build_system_prompt(self) -> str:\r\n        """Build system prompt with robot capabilities"""\r\n        prompt = """You are a robot task planner. Convert natural language commands into structured action sequences.\r\n\r\nAvailable actions:\r\n"""\r\n        for action in self.capabilities["actions"]:\r\n            prompt += f"\\n- {action[\'name\']}: {action[\'description\']}"\r\n            prompt += f"\\n  Parameters: {action[\'parameters\']}"\r\n        \r\n        prompt += "\\n\\nKnown locations:\\n"\r\n        for loc, coords in self.capabilities["known_locations"].items():\r\n            prompt += f"- {loc}: ({coords[\'x\']}, {coords[\'y\']})\\n"\r\n        \r\n        prompt += """\r\nRespond with JSON in this format:\r\n{\r\n  "task_summary": "Brief description of the task",\r\n  "steps": [\r\n    {\r\n      "step": 1,\r\n      "action": "action_name",\r\n      "parameters": {...},\r\n      "reasoning": "Why this step is needed"\r\n    }\r\n  ],\r\n  "estimated_duration": "time in seconds"\r\n}\r\n\r\nImportant:\r\n1. Break complex tasks into simple actions\r\n2. Include navigation between locations\r\n3. Scan for objects before manipulation\r\n4. Validate object availability\r\n5. Handle edge cases (e.g., object not found)\r\n"""\r\n        return prompt\r\n    \r\n    def validate_plan(self, plan: Dict[str, Any]) -> bool:\r\n        """\r\n        Validate that plan is executable.\r\n        \r\n        Args:\r\n            plan: Generated plan dictionary\r\n            \r\n        Returns:\r\n            bool: True if valid, False otherwise\r\n        """\r\n        if "steps" not in plan:\r\n            self.get_logger().error(\'Plan missing "steps" field\')\r\n            return False\r\n        \r\n        # Validate each step\r\n        for step in plan["steps"]:\r\n            if "action" not in step:\r\n                self.get_logger().error(f\'Step {step.get("step", "?")} missing action\')\r\n                return False\r\n            \r\n            action_name = step["action"]\r\n            \r\n            # Check if action exists\r\n            valid_actions = [a["name"] for a in self.capabilities["actions"]]\r\n            if action_name not in valid_actions:\r\n                self.get_logger().error(f\'Unknown action: {action_name}\')\r\n                return False\r\n            \r\n            # Validate parameters\r\n            # (Add more sophisticated validation here)\r\n        \r\n        return True\r\n    \r\n    def execute_plan(self, plan: Dict[str, Any]):\r\n        """\r\n        Execute the generated plan.\r\n        \r\n        Args:\r\n            plan: Validated action plan\r\n        """\r\n        self.publish_status(f"Executing plan: {plan.get(\'task_summary\', \'Unknown task\')}")\r\n        \r\n        for step in plan["steps"]:\r\n            step_num = step["step"]\r\n            action = step["action"]\r\n            params = step.get("parameters", {})\r\n            reasoning = step.get("reasoning", "")\r\n            \r\n            self.get_logger().info(\r\n                f"Step {step_num}: {action} - {reasoning}"\r\n            )\r\n            \r\n            # Execute action\r\n            success = self.execute_action(action, params)\r\n            \r\n            if not success:\r\n                self.get_logger().error(f\'Step {step_num} failed\')\r\n                self.publish_status(f"Plan failed at step {step_num}")\r\n                return\r\n            \r\n            self.publish_status(f"Completed step {step_num}/{len(plan[\'steps\'])}")\r\n        \r\n        self.get_logger().info(\'Plan execution completed successfully\')\r\n        self.publish_status("Task completed successfully")\r\n    \r\n    def execute_action(self, action: str, params: Dict[str, Any]) -> bool:\r\n        """\r\n        Execute a single action.\r\n        \r\n        Args:\r\n            action: Action name\r\n            params: Action parameters\r\n            \r\n        Returns:\r\n            bool: True if successful\r\n        """\r\n        if action == "navigate":\r\n            return self.execute_navigate(params)\r\n        elif action == "pick":\r\n            return self.execute_pick(params)\r\n        elif action == "place":\r\n            return self.execute_place(params)\r\n        elif action == "scan":\r\n            return self.execute_scan(params)\r\n        elif action == "wait":\r\n            return self.execute_wait(params)\r\n        else:\r\n            self.get_logger().error(f\'Unknown action: {action}\')\r\n            return False\r\n    \r\n    def execute_navigate(self, params: Dict[str, Any]) -> bool:\r\n        """Execute navigation action"""\r\n        x = params.get(\'x\', 0.0)\r\n        y = params.get(\'y\', 0.0)\r\n        theta = params.get(\'theta\', 0.0)\r\n        \r\n        self.get_logger().info(f\'Navigating to ({x}, {y}, {theta})\')\r\n        \r\n        # Send goal to navigation action server\r\n        goal = NavigateToPoint.Goal()\r\n        goal.target_x = x\r\n        goal.target_y = y\r\n        goal.target_theta = theta\r\n        \r\n        future = self.nav_client.send_goal_async(goal)\r\n        rclpy.spin_until_future_complete(self, future)\r\n        \r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().error(\'Navigation goal rejected\')\r\n            return False\r\n        \r\n        # Wait for result\r\n        result_future = goal_handle.get_result_async()\r\n        rclpy.spin_until_future_complete(self, result_future)\r\n        \r\n        result = result_future.result().result\r\n        return result.success\r\n    \r\n    def execute_pick(self, params: Dict[str, Any]) -> bool:\r\n        """Execute pick action"""\r\n        object_name = params.get(\'object_name\', \'\')\r\n        self.get_logger().info(f\'Picking object: {object_name}\')\r\n        \r\n        # Implementation: call manipulation action server\r\n        # ...\r\n        return True\r\n    \r\n    def execute_place(self, params: Dict[str, Any]) -> bool:\r\n        """Execute place action"""\r\n        x = params.get(\'x\', 0.0)\r\n        y = params.get(\'y\', 0.0)\r\n        self.get_logger().info(f\'Placing object at ({x}, {y})\')\r\n        \r\n        # Implementation: call manipulation action server\r\n        # ...\r\n        return True\r\n    \r\n    def execute_scan(self, params: Dict[str, Any]) -> bool:\r\n        """Execute environment scan"""\r\n        self.get_logger().info(\'Scanning environment\')\r\n        \r\n        # Implementation: trigger perception pipeline\r\n        # ...\r\n        return True\r\n    \r\n    def execute_wait(self, params: Dict[str, Any]) -> bool:\r\n        """Execute wait action"""\r\n        duration = params.get(\'duration\', 1.0)\r\n        self.get_logger().info(f\'Waiting for {duration} seconds\')\r\n        \r\n        import time\r\n        time.sleep(duration)\r\n        return True\r\n    \r\n    def publish_status(self, status: str):\r\n        """Publish plan execution status"""\r\n        msg = String()\r\n        msg.data = status\r\n        self.status_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    planner = CognitivePlanner()\r\n    \r\n    try:\r\n        rclpy.spin(planner)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        planner.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,i.jsx)(e.h3,{id:"key-principles",children:"Key Principles"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Be Specific About Capabilities"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"List all available actions"}),"\n",(0,i.jsx)(e.li,{children:"Specify parameter types and ranges"}),"\n",(0,i.jsx)(e.li,{children:"Include constraints"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Provide Context"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Current robot state"}),"\n",(0,i.jsx)(e.li,{children:"Environment information"}),"\n",(0,i.jsx)(e.li,{children:"Known locations and objects"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Request Structured Output"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use JSON format"}),"\n",(0,i.jsx)(e.li,{children:"Define schema clearly"}),"\n",(0,i.jsx)(e.li,{children:"Include reasoning fields"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Handle Uncertainty"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Ask for confidence scores"}),"\n",(0,i.jsx)(e.li,{children:"Request alternative plans"}),"\n",(0,i.jsx)(e.li,{children:"Include error handling"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"example-prompts",children:"Example Prompts"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Basic Task:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Task: Go to the kitchen and bring me a cup\r\n\r\nCurrent state:\r\n- Location: living_room\r\n- Holding: nothing\r\n- Battery: 90%\r\n\r\nKnown objects:\r\n- cup: kitchen_counter\r\n- plate: kitchen_counter\r\n- book: living_room_table\r\n\r\nGenerate a plan.\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Complex Task with Constraints:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'Task: Clean the living room but avoid the area near the TV\r\n\r\nConstraints:\r\n- Do not approach within 1 meter of TV (at x=2.0, y=3.0)\r\n- Only pick up objects classified as "trash"\r\n- Return to charging station if battery < 20%\r\n\r\nCurrent state:\r\n- Location: charging_station\r\n- Battery: 85%\r\n- Holding: nothing\r\n\r\nGenerate a plan with safety considerations.\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"advanced-multi-turn-planning",children:"Advanced: Multi-Turn Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class MultiTurnPlanner(Node):\r\n    """\r\n    Maintains conversation history for iterative planning.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'multi_turn_planner\')\r\n        self.conversation_history = []\r\n        self.current_plan = None\r\n    \r\n    def generate_plan_with_history(self, command: str):\r\n        """Generate plan considering conversation history"""\r\n        # Add user command to history\r\n        self.conversation_history.append({\r\n            "role": "user",\r\n            "content": command\r\n        })\r\n        \r\n        # Generate plan\r\n        response = self.client.chat.completions.create(\r\n            model=self.model,\r\n            messages=[\r\n                {"role": "system", "content": self.system_prompt},\r\n                *self.conversation_history\r\n            ]\r\n        )\r\n        \r\n        # Add assistant response to history\r\n        self.conversation_history.append({\r\n            "role": "assistant",\r\n            "content": response.choices[0].message.content\r\n        })\r\n        \r\n        return json.loads(response.choices[0].message.content)\r\n    \r\n    def handle_clarification(self, user_response: str):\r\n        """Handle user clarification or modification"""\r\n        self.conversation_history.append({\r\n            "role": "user",\r\n            "content": user_response\r\n        })\r\n        \r\n        # Regenerate plan with clarification\r\n        return self.generate_plan_with_history("Update the plan based on my feedback")\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Example Conversation:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'User: "Clean the room"\r\nRobot: "I found 3 items. Should I throw away the paper on the floor?"\r\nUser: "No, that\'s important. Just throw away the plastic bottles."\r\nRobot: "Understood. Updating plan to only remove plastic bottles."\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def execute_plan_with_recovery(self, plan):\r\n    """Execute plan with error recovery"""\r\n    for step in plan["steps"]:\r\n        max_retries = 3\r\n        retry_count = 0\r\n        \r\n        while retry_count < max_retries:\r\n            success = self.execute_action(step["action"], step["parameters"])\r\n            \r\n            if success:\r\n                break\r\n            else:\r\n                retry_count += 1\r\n                self.get_logger().warn(f\'Step failed, retry {retry_count}/{max_retries}\')\r\n                \r\n                if retry_count >= max_retries:\r\n                    # Ask LLM for recovery plan\r\n                    recovery_plan = self.generate_recovery_plan(step, plan)\r\n                    if recovery_plan:\r\n                        self.execute_plan(recovery_plan)\r\n                    else:\r\n                        return False\r\n    \r\n    return True\r\n\r\ndef generate_recovery_plan(self, failed_step, original_plan):\r\n    """Ask LLM to generate recovery plan"""\r\n    prompt = f"""\r\nThe following step failed:\r\n{json.dumps(failed_step, indent=2)}\r\n\r\nOriginal plan:\r\n{json.dumps(original_plan, indent=2)}\r\n\r\nGenerate a recovery plan to either:\r\n1. Retry with different parameters\r\n2. Skip this step and continue\r\n3. Abort and return to safe state\r\n\r\nConsider what might have gone wrong and how to recover.\r\n"""\r\n    \r\n    # Call LLM for recovery plan\r\n    # ...\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(e.h3,{id:"-do",children:"\u2705 DO:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Validate LLM Output"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Check JSON structure"}),"\n",(0,i.jsx)(e.li,{children:"Verify action names"}),"\n",(0,i.jsx)(e.li,{children:"Validate parameters"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Provide Rich Context"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Current robot state"}),"\n",(0,i.jsx)(e.li,{children:"Environment information"}),"\n",(0,i.jsx)(e.li,{children:"Previous actions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Use Low Temperature"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"0.2-0.4 for deterministic planning"}),"\n",(0,i.jsx)(e.li,{children:"Higher for creative tasks"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Implement Timeouts"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"LLM calls can be slow"}),"\n",(0,i.jsx)(e.li,{children:"Have fallback behaviors"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Log Everything"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Save prompts and responses"}),"\n",(0,i.jsx)(e.li,{children:"Useful for debugging and improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"-dont",children:"\u274c DON'T:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Don't Trust Blindly"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Always validate LLM output"}),"\n",(0,i.jsx)(e.li,{children:"Implement safety checks"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Don't Expose Sensitive Info"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Be careful with API keys"}),"\n",(0,i.jsx)(e.li,{children:"Don't send private data to LLM"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Don't Ignore Failures"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Handle LLM errors gracefully"}),"\n",(0,i.jsx)(e.li,{children:"Have fallback strategies"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"LLMs enable natural language robot control"}),"\n",(0,i.jsx)(e.li,{children:"Prompt engineering is critical for success"}),"\n",(0,i.jsx)(e.li,{children:"Structured output (JSON) is essential"}),"\n",(0,i.jsx)(e.li,{children:"Validation and safety checks are mandatory"}),"\n",(0,i.jsx)(e.li,{children:"Multi-turn conversations enable refinement"}),"\n",(0,i.jsx)(e.li,{children:"Error recovery improves robustness"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"LLM-based cognitive planning transforms how humans interact with robots, enabling intuitive, flexible task specification."}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://platform.openai.com/docs",children:"OpenAI API Documentation"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://www.promptingguide.ai/",children:"Prompt Engineering Guide"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://say-can.github.io/",children:"SayCan: Grounding Language in Robotic Affordances"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://code-as-policies.github.io/",children:"Code as Policies"})}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>l});var t=r(6540);const i={},a=t.createContext(i);function s(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);