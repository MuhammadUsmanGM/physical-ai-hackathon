"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[467],{8453:(n,r,e)=>{e.d(r,{R:()=>t,x:()=>a});var o=e(6540);const s={},i=o.createContext(s);function t(n){const r=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(r):{...r,...n}},[r,n])}function a(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),o.createElement(i.Provider,{value:r},n.children)}},9330:(n,r,e)=>{e.r(r),e.d(r,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"The-AI-Robot-Brain/RL-Robot-Control/rl-robot-control","title":"Reinforcement Learning for Robot Control","description":"Introduction","source":"@site/docs/04-The-AI-Robot-Brain/04-RL-Robot-Control/index.md","sourceDirName":"04-The-AI-Robot-Brain/04-RL-Robot-Control","slug":"/module-04/rl-robot-control","permalink":"/physical-ai-hackathon/docs/module-04/rl-robot-control","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04-The-AI-Robot-Brain/04-RL-Robot-Control/index.md","tags":[],"version":"current","frontMatter":{"id":"rl-robot-control","title":"Reinforcement Learning for Robot Control","slug":"/module-04/rl-robot-control"},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Bipedal Navigation","permalink":"/physical-ai-hackathon/docs/module-04/nav2-bipedal-navigation"},"next":{"title":"Deploying to Jetson Hardware","permalink":"/physical-ai-hackathon/docs/module-04/jetson-deployment"}}');var s=e(4848),i=e(8453);const t={id:"rl-robot-control",title:"Reinforcement Learning for Robot Control",slug:"/module-04/rl-robot-control"},a="Reinforcement Learning for Robot Control",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"RL Fundamentals",id:"rl-fundamentals",level:2},{value:"PPO for Humanoid Locomotion",id:"ppo-for-humanoid-locomotion",level:2},{value:"Complete Implementation",id:"complete-implementation",level:3},{value:"Reward Shaping",id:"reward-shaping",level:2},{value:"Reward Components",id:"reward-components",level:3},{value:"Curriculum Learning",id:"curriculum-learning",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"reinforcement-learning-for-robot-control",children:"Reinforcement Learning for Robot Control"})}),"\n",(0,s.jsx)(r.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Reinforcement Learning (RL)"})," enables robots to learn complex behaviors through trial and error. This chapter covers RL fundamentals and practical implementation for humanoid robot control using modern frameworks."]}),"\n",(0,s.jsx)(r.h2,{id:"rl-fundamentals",children:"RL Fundamentals"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-mermaid",children:"graph LR\r\n    AGENT[Agent/Policy] --\x3e ACTION[Action]\r\n    ACTION --\x3e ENV[Environment]\r\n    ENV --\x3e STATE[State]\r\n    ENV --\x3e REWARD[Reward]\r\n    STATE --\x3e AGENT\r\n    REWARD --\x3e AGENT\n"})}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.strong,{children:"Key Components:"})}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Agent:"})," The learning algorithm (neural network policy)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Environment:"})," The robot and its world (simulation or real)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"State:"})," Sensor observations (joint positions, IMU, camera, etc.)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Action:"})," Motor commands (joint torques or velocities)"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Reward:"})," Feedback signal (task progress, penalties for falling, etc.)"]}),"\n"]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"ppo-for-humanoid-locomotion",children:"PPO for Humanoid Locomotion"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Proximal Policy Optimization (PPO)"})," is the most popular RL algorithm for robotics."]}),"\n",(0,s.jsx)(r.h3,{id:"complete-implementation",children:"Complete Implementation"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nPPO training for humanoid walking using Stable Baselines3\r\n"""\r\n\r\nimport gym\r\nfrom gym import spaces\r\nimport numpy as np\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState, Imu\r\nfrom std_msgs.msg import Float64MultiArray\r\n\r\nfrom stable_baselines3 import PPO\r\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\r\nfrom stable_baselines3.common.callbacks import CheckpointCallback\r\n\r\nclass HumanoidWalkEnv(gym.Env, Node):\r\n    """\r\n    Gym environment for humanoid walking.\r\n    """\r\n    \r\n    def __init__(self):\r\n        gym.Env.__init__(self)\r\n        Node.__init__(self, \'humanoid_walk_env\')\r\n        \r\n        # Action space: 12 joint torques\r\n        self.action_space = spaces.Box(\r\n            low=-50.0,\r\n            high=50.0,\r\n            shape=(12,),\r\n            dtype=np.float32\r\n        )\r\n        \r\n        # Observation space: 30 dimensions\r\n        # - 12 joint positions\r\n        # - 12 joint velocities\r\n        # - 3 IMU angular velocities\r\n        # - 3 IMU linear accelerations\r\n        self.observation_space = spaces.Box(\r\n            low=-np.inf,\r\n            high=np.inf,\r\n            shape=(30,),\r\n            dtype=np.float32\r\n        )\r\n        \r\n        # State variables\r\n        self.joint_positions = np.zeros(12)\r\n        self.joint_velocities = np.zeros(12)\r\n        self.imu_angular_vel = np.zeros(3)\r\n        self.imu_linear_accel = np.zeros(3)\r\n        self.base_position = np.zeros(3)\r\n        \r\n        # Target velocity\r\n        self.target_velocity = 0.5  # m/s forward\r\n        \r\n        # Episode tracking\r\n        self.episode_steps = 0\r\n        self.max_episode_steps = 1000\r\n        \r\n        # ROS interfaces\r\n        self.setup_ros_interfaces()\r\n        \r\n        self.get_logger().info(\'Humanoid Walk Environment initialized\')\r\n    \r\n    def setup_ros_interfaces(self):\r\n        """Setup ROS publishers and subscribers"""\r\n        # Subscribe to joint states\r\n        self.joint_sub = self.create_subscription(\r\n            JointState,\r\n            \'/joint_states\',\r\n            self.joint_callback,\r\n            10\r\n        )\r\n        \r\n        # Subscribe to IMU\r\n        self.imu_sub = self.create_subscription(\r\n            Imu,\r\n            \'/imu/data\',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n        \r\n        # Publish joint commands\r\n        self.cmd_pub = self.create_publisher(\r\n            Float64MultiArray,\r\n            \'/joint_commands\',\r\n            10\r\n        )\r\n    \r\n    def joint_callback(self, msg):\r\n        """Update joint state"""\r\n        self.joint_positions = np.array(msg.position)\r\n        self.joint_velocities = np.array(msg.velocity)\r\n    \r\n    def imu_callback(self, msg):\r\n        """Update IMU data"""\r\n        self.imu_angular_vel = np.array([\r\n            msg.angular_velocity.x,\r\n            msg.angular_velocity.y,\r\n            msg.angular_velocity.z\r\n        ])\r\n        self.imu_linear_accel = np.array([\r\n            msg.linear_acceleration.x,\r\n            msg.linear_acceleration.y,\r\n            msg.linear_acceleration.z\r\n        ])\r\n    \r\n    def reset(self):\r\n        """Reset environment to initial state"""\r\n        # Reset simulation\r\n        self.reset_simulation()\r\n        \r\n        # Wait for sensors to update\r\n        rclpy.spin_once(self, timeout_sec=0.1)\r\n        \r\n        # Reset episode tracking\r\n        self.episode_steps = 0\r\n        \r\n        # Return initial observation\r\n        return self.get_observation()\r\n    \r\n    def step(self, action):\r\n        """\r\n        Execute action and return next state, reward, done, info.\r\n        \r\n        Args:\r\n            action: Joint torques (12,)\r\n            \r\n        Returns:\r\n            observation, reward, done, info\r\n        """\r\n        # Apply action (joint torques)\r\n        self.apply_action(action)\r\n        \r\n        # Step simulation\r\n        rclpy.spin_once(self, timeout_sec=0.02)  # 50 Hz\r\n        \r\n        # Get new observation\r\n        obs = self.get_observation()\r\n        \r\n        # Calculate reward\r\n        reward = self.compute_reward()\r\n        \r\n        # Check if episode is done\r\n        done = self.is_done()\r\n        \r\n        # Additional info\r\n        info = {\r\n            \'episode_steps\': self.episode_steps,\r\n            \'base_height\': self.base_position[2]\r\n        }\r\n        \r\n        self.episode_steps += 1\r\n        \r\n        return obs, reward, done, info\r\n    \r\n    def get_observation(self):\r\n        """\r\n        Get current observation.\r\n        \r\n        Returns:\r\n            np.array: Observation vector (30,)\r\n        """\r\n        obs = np.concatenate([\r\n            self.joint_positions,      # 12\r\n            self.joint_velocities,     # 12\r\n            self.imu_angular_vel,      # 3\r\n            self.imu_linear_accel      # 3\r\n        ])\r\n        \r\n        return obs.astype(np.float32)\r\n    \r\n    def apply_action(self, action):\r\n        """\r\n        Apply joint torques.\r\n        \r\n        Args:\r\n            action: Joint torques (12,)\r\n        """\r\n        # Clip to limits\r\n        action = np.clip(action, -50.0, 50.0)\r\n        \r\n        # Publish to ROS\r\n        msg = Float64MultiArray()\r\n        msg.data = action.tolist()\r\n        self.cmd_pub.publish(msg)\r\n    \r\n    def compute_reward(self):\r\n        """\r\n        Compute reward for current state.\r\n        \r\n        Returns:\r\n            float: Reward value\r\n        """\r\n        reward = 0.0\r\n        \r\n        # 1. Forward velocity reward\r\n        forward_vel = self.get_forward_velocity()\r\n        velocity_reward = -abs(forward_vel - self.target_velocity)\r\n        reward += velocity_reward\r\n        \r\n        # 2. Alive bonus (encourage staying upright)\r\n        if self.base_position[2] > 0.8:  # Height > 80cm\r\n            reward += 1.0\r\n        \r\n        # 3. Energy penalty (encourage efficiency)\r\n        energy = np.sum(np.abs(self.joint_velocities))\r\n        reward -= 0.01 * energy\r\n        \r\n        # 4. Stability reward (penalize excessive tilting)\r\n        tilt = np.sqrt(self.imu_angular_vel[0]**2 + self.imu_angular_vel[1]**2)\r\n        reward -= 0.1 * tilt\r\n        \r\n        return reward\r\n    \r\n    def is_done(self):\r\n        """\r\n        Check if episode should terminate.\r\n        \r\n        Returns:\r\n            bool: True if done\r\n        """\r\n        # Terminate if fallen\r\n        if self.base_position[2] < 0.5:  # Height < 50cm\r\n            return True\r\n        \r\n        # Terminate if max steps reached\r\n        if self.episode_steps >= self.max_episode_steps:\r\n            return True\r\n        \r\n        # Terminate if tilted too much\r\n        if abs(self.imu_angular_vel[0]) > 2.0 or abs(self.imu_angular_vel[1]) > 2.0:\r\n            return True\r\n        \r\n        return False\r\n    \r\n    def get_forward_velocity(self):\r\n        """Get forward velocity from IMU"""\r\n        # Simplified: use linear acceleration\r\n        return self.imu_linear_accel[0]\r\n    \r\n    def reset_simulation(self):\r\n        """Reset Gazebo simulation"""\r\n        # Call Gazebo reset service\r\n        # ...\r\n        pass\r\n\r\ndef train_humanoid_walking():\r\n    """Train humanoid walking policy"""\r\n    # Create environment\r\n    env = DummyVecEnv([lambda: HumanoidWalkEnv()])\r\n    \r\n    # Create PPO agent\r\n    model = PPO(\r\n        \'MlpPolicy\',\r\n        env,\r\n        learning_rate=3e-4,\r\n        n_steps=2048,\r\n        batch_size=64,\r\n        n_epochs=10,\r\n        gamma=0.99,\r\n        gae_lambda=0.95,\r\n        clip_range=0.2,\r\n        ent_coef=0.0,\r\n        verbose=1,\r\n        tensorboard_log=\'./logs/\'\r\n    )\r\n    \r\n    # Checkpoint callback\r\n    checkpoint_callback = CheckpointCallback(\r\n        save_freq=10000,\r\n        save_path=\'./models/\',\r\n        name_prefix=\'humanoid_walk\'\r\n    )\r\n    \r\n    # Train\r\n    model.learn(\r\n        total_timesteps=1_000_000,\r\n        callback=checkpoint_callback\r\n    )\r\n    \r\n    # Save final model\r\n    model.save(\'humanoid_walk_final\')\r\n    \r\n    return model\r\n\r\ndef test_policy(model_path):\r\n    """Test trained policy"""\r\n    # Load model\r\n    model = PPO.load(model_path)\r\n    \r\n    # Create environment\r\n    env = HumanoidWalkEnv()\r\n    \r\n    # Test for 10 episodes\r\n    for episode in range(10):\r\n        obs = env.reset()\r\n        done = False\r\n        episode_reward = 0\r\n        \r\n        while not done:\r\n            # Get action from policy\r\n            action, _ = model.predict(obs, deterministic=True)\r\n            \r\n            # Step environment\r\n            obs, reward, done, info = env.step(action)\r\n            episode_reward += reward\r\n        \r\n        print(f\'Episode {episode}: Reward = {episode_reward:.2f}\')\r\n\r\nif __name__ == \'__main__\':\r\n    rclpy.init()\r\n    \r\n    # Train\r\n    model = train_humanoid_walking()\r\n    \r\n    # Test\r\n    test_policy(\'humanoid_walk_final\')\r\n    \r\n    rclpy.shutdown()\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"reward-shaping",children:"Reward Shaping"}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.strong,{children:"Good reward design is critical for RL success."})}),"\n",(0,s.jsx)(r.h3,{id:"reward-components",children:"Reward Components"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'def compute_shaped_reward(self):\r\n    """Well-designed reward function"""\r\n    reward = 0.0\r\n    \r\n    # 1. Task progress (primary objective)\r\n    forward_vel = self.get_forward_velocity()\r\n    reward += 2.0 * min(forward_vel / self.target_velocity, 1.0)\r\n    \r\n    # 2. Alive bonus\r\n    reward += 0.5\r\n    \r\n    # 3. Energy efficiency\r\n    torque_penalty = np.sum(np.square(self.last_action)) / 12\r\n    reward -= 0.001 * torque_penalty\r\n    \r\n    # 4. Smooth motion\r\n    jerk = np.sum(np.abs(self.joint_velocities - self.prev_joint_velocities))\r\n    reward -= 0.01 * jerk\r\n    \r\n    # 5. Foot contact (encourage proper gait)\r\n    if self.is_foot_in_contact(\'left\') != self.is_foot_in_contact(\'right\'):\r\n        reward += 0.1  # Reward alternating foot contact\r\n    \r\n    # 6. Upright posture\r\n    torso_angle = self.get_torso_angle()\r\n    reward -= 0.5 * abs(torso_angle)\r\n    \r\n    return reward\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"curriculum-learning",children:"Curriculum Learning"}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.strong,{children:"Gradually increase task difficulty."})}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"class CurriculumManager:\r\n    \"\"\"\r\n    Manages curriculum learning for humanoid training.\r\n    \"\"\"\r\n    \r\n    def __init__(self):\r\n        self.current_level = 0\r\n        self.levels = [\r\n            {'target_velocity': 0.2, 'terrain': 'flat'},\r\n            {'target_velocity': 0.4, 'terrain': 'flat'},\r\n            {'target_velocity': 0.6, 'terrain': 'flat'},\r\n            {'target_velocity': 0.6, 'terrain': 'rough'},\r\n            {'target_velocity': 0.8, 'terrain': 'rough'},\r\n        ]\r\n        \r\n        self.success_threshold = 0.8\r\n        self.success_window = 100\r\n        self.recent_successes = []\r\n    \r\n    def update(self, episode_success):\r\n        \"\"\"Update curriculum based on performance\"\"\"\r\n        self.recent_successes.append(episode_success)\r\n        \r\n        # Keep only recent history\r\n        if len(self.recent_successes) > self.success_window:\r\n            self.recent_successes.pop(0)\r\n        \r\n        # Check if should advance\r\n        if len(self.recent_successes) == self.success_window:\r\n            success_rate = sum(self.recent_successes) / self.success_window\r\n            \r\n            if success_rate > self.success_threshold:\r\n                if self.current_level < len(self.levels) - 1:\r\n                    self.current_level += 1\r\n                    self.recent_successes = []\r\n                    print(f'Advanced to level {self.current_level}')\r\n    \r\n    def get_current_config(self):\r\n        \"\"\"Get current curriculum configuration\"\"\"\r\n        return self.levels[self.current_level]\n"})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"RL enables learning complex behaviors"})," through trial and error"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"PPO is the standard"})," for robot control"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Reward shaping is critical"})," for success"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Curriculum learning"})," speeds up training"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Sim-to-real transfer"})," requires domain randomization"]}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"RL is transforming how we develop robot controllers, enabling behaviors that are difficult to hand-engineer."}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://stable-baselines3.readthedocs.io/",children:"Stable Baselines3 Documentation"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://spinningup.openai.com/",children:"Spinning Up in Deep RL"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://arxiv.org/abs/1808.00177",children:"Learning Dexterous In-Hand Manipulation"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://arxiv.org/abs/1707.06347",children:"PPO Paper"})}),"\n"]})]})}function u(n={}){const{wrapper:r}={...(0,i.R)(),...n.components};return r?(0,s.jsx)(r,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);